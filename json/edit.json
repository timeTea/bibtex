{
    "id":"2",
    "keyWords":["bert","self-attention"],
    "memo":"i like it",
    "modelAlgo":"bert",
    "biborigin":"@inproceedings{DevlinCLT19,\n        author    = {Jacob Devlin and\n                     Ming{-}Wei Chang and\n                     Kenton Lee and\n                     Kristina Toutanova},\n        editor    = {Jill Burstein and\n                     Christy Doran and\n                     Thamar Solorio},\n        title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n                     Understanding},\n        booktitle = {Proceedings of the 2019 Conference of the North American Chapter of\n                     the Association for Computational Linguistics: Human Language Technologies,\n                     {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long\n                     and Short Papers)},\n        pages     = {4171--4186},\n        publisher = {Association for Computational Linguistics},\n        year      = {2019},\n        url       = {https://doi.org/10.18653/v1/n19-1423},\n        doi       = {10.18653/v1/n19-1423},\n        timestamp = {Tue, 28 Jan 2020 10:30:29 +0100},\n        biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},\n        bibsource = {dblp computer science bibliography, https://dblp.org}\n      }"
}